#!/bin/env python

'''
Evaluate presence of content synchronized to Coordinating Nodes.

Retrieve PIDs, SIDs, docid, revisions, and file names directly from CN
databases and file system, then perform some analysis of the retrieved
content to verify consistency across CNs.

This script bypasses the DataONE APIs to retrieve identifiers directly from the
postgres database operating on Coordinating Nodes. Shell access to the CNs and
a (readonly) postgres account are required. No password is provided for here,
so access control for ssh is via a key pair, and for postgres via a .pgpass file
or similar mechanism.

By default, content is placed in a subfolder named getpids_{env}_{date} of the
current working folder, where env is the name of the environment being examined
and date is the current date (UTC).

Analyses

1. Files that are present on one node but not on another

2. PIDs that are on one node but not on another


Output

Several files are created in the output subfolder:

cndata.sqlite

An sqlite3 database that contains tables::

  CREATE TABLE metadata (
    k TEXT,
    v TEXT
  );

  CREATE TABLE identifier (
      host TEXT, 
      pid TEXT, 
      autogen TEXT,
      rev INTEGER, 
      docid TEXT, 
      nodeid TEXT, 
      formatid TEXT
  );
  CREATE TABLE files (
      host TEXT, 
      filename TEXT
  );


data_files_{HOST}.txt

Contains a list of all the autogen ids for all documents in the metacat data folder.

identifiers_{HOST}.txt

Contains a space delimited list of:

  identifier, autogen_id, revison_id, node_id

for identifiers in the metacat postgres database that match the provided constraints (a combination of start time, end time, node id).

analysis.json

results.json



Requires

* d1_admin_tools https://github.com/DataONEorg/DataONE_Operations
* dateparser https://pypi.python.org/pypi/dateparser
* fabric http://www.fabfile.org/


'''

import sys
import logging
import argparse
import d1_admin_tools
from d1_admin_tools import operations
import datetime
import dateparser
import os
import csv
import codecs
import json
import sqlite3
from fabric.api import task, execute, run, get
from jinja2 import Environment, FileSystemLoader, select_autoescape


PSQL_READONLY_USER = "dataone_readonly"
PSQL_DATE_FORMAT="%Y-%m-%dT%H:%M:%S.%f%z"
RESULTS_FILE = 'results.json'
ANALYSIS_FILE = 'analysis.json'

DOCID_ANALYSIS = 'docid_analysis.json'
PID_ANALYSIS = 'pid_analysis.json'

#==================
#++ Fabric Tasks ++

def runSQLQuery(SQL, dest_file, tmp_file=None, user=PSQL_READONLY_USER, database='metacat', delimiter=' '):
  '''
  Fabric task that executes SQL on postgres and retrieves the resulting output as a file.

  :param SQL: SQL to run, expects a table output
  :param dest_file: Local file that will contain the results
  :param tmp_file: Temporary file on remote system. Created if not specified.
  :return: Nothing
  '''
  logger = logging.getLogger('main')
  if tmp_file is None:
    tmp_file = operations.tmpFileName("/tmp/", prefix="sql", ext="txt")
  SQL = "COPY (" + SQL + ") TO STDOUT WITH (FORMAT CSV, HEADER FALSE, FORCE_QUOTE *);"
  cmd = "psql -h localhost -U " + PSQL_READONLY_USER + " " + database
  cmd += " -P pager=off --single-transaction"
  cmd += " --no-align -t --field-separator '" + delimiter + "' --quiet"
  cmd += " -o " + tmp_file + " -c \"" + SQL + ";\""
  logger.info("PSQL Command = %s", cmd)
  run(cmd)
  get(tmp_file, dest_file)
  run("rm " + tmp_file)


def listDataDocuments(dest_file):
  '''
  Fabric task to retrieve a list of files managed by metacat on a CN.

  :param dest_file: File to contain results, one filename per line
  :return: Nothing
  '''
  tmp_file = operations.tmpFileName("/tmp/", prefix="metacat_files", ext="txt")
  cmd = "ls /var/metacat/documents > {0}".format(tmp_file)
  run(cmd)
  cmd = "ls /var/metacat/data >> {0}".format(tmp_file)
  run(cmd)
  get(tmp_file, dest_file)
  run("rm " + tmp_file)


#==================

class CNodeComparison(object):
  
  def __init__(self, dest_folder):
    self.dest_folder = dest_folder
    self.db_file = os.path.join(dest_folder, "cndata.sqlite")
    self.dbc = None
    self.createLocalDatabase()
    
    
  def getDatabaseConnection(self, force=False):
    if not self.dbc is None and not force:
      return self.dbc
    self.dbc = sqlite3.connect( self.db_file )


  def createLocalDatabase(self):
    dbc = self.getDatabaseConnection()
    cur = dbc.cursor()
    cur.execute("CREATE TABLE IF NOT EXISTS metadata (k TEXT, v TEXT)")
    cur.execute("""CREATE TABLE IF NOT EXISTS identifier (host TEXT, pid TEXT,
      autogen TEXT, rev INTEGER, docid TEXT, nodeid TEXT, formatid TEXT);""")
    cur.execute("""CREATE INDEX IF NOT EXISTS idx_identifier ON identifier 
      (pid, host, docid);""")
    cur.execute("CREATE TABLE IF NOT EXISTS files (host TEXT, filename TEXT);")
    cur.execute("""CREATE INDEX IF NOT EXISTS idx_files ON files 
      (filename, host);""")
    dbc.commit()
    dbc.close()


  def getMetadata(self, k):
    dbc = self.getDatabaseConnection()
    cur = dbc.cursor()
    cur.execute("SELECT v FROM metadata WHERE k=?", [k])
    return json.loads(cur.fetchone()[0])


  def deleteMetadata(self, k):
    if self.getMetadata(k) is None:
      return
    SQL = "DELETE FROM metadata WHERE k=?"
    dbc = self.getDatabaseConnection()
    cur = dbc.cursor()
    cur.execute(SQL, [k] )
    dbc.commit()


  def setMetadata(self, k, v):
    dbc = self.getDatabaseConnection()
    cur = dbc.cursor()
    v_encoded = json.dumps(v, encoding='utf-8')
    if self.getMetadata(k) is None:
      SQL = "INSERT INTO metadata (?, ?)"
      cur.execute(SQL, [k, v])
    else:
      SQL = "UPDATE metadata SET v=?"
      cur.execute(SQL, v)
    dbc.commit()


  def writeJsonFile(self, fname, data):
    with codecs.open( os.path.join(self.dest_folder, fname), 
                      mode='wb', 
                      encoding='utf-8' ) as rfile:
      rfile.write( json.dumps(data, 
                              encoding='utf-8', 
                              sort_keys=True, indent=2 ))
    
  
  def readJsonFile(self, fname):
    with codecs.open( os.path.join(self.dest_folder, fname), 
                      mode='rb', 
                      encoding='utf-8' ) as rfile:
      return json.loads( rfile )


  def getDocids(self, hosts):
    '''
    Retrieve a list of files from /var/metacat/data and
      /var/metacat/documents from each of hosts.

    Output is written to files in dest_folder named data_files_{host}. 
    Each output file contains one entry per line.

    :param results:
    :param hosts:
    :param dest_folder:
    :return:
    '''
    logger = logging.getLogger('main')
    results['results']['data_documents'] = {}
    dbc = self.getDatabaseConnection()
    cur = dbc.cursor()
    for host in hosts:
      dest_file = "data_files_{0}.txt".format(host))
      self.setMetadata("docids.{0}".format(host), dest_file)
      execute(listDataDocuments, 
              os.path.join(self.dest_folder, dest_file), 
              host=host)
      logger.info("Adding document list for %s to database...", host)
      with open(dest_file, 'r') as infile:
        for row in infile.readlines(): 
          cur.execute(u"INSERT INTO files VALUES (?, ?)", 
                      [ host, toUnicode(row.strip()) ] )
      dbc.commit()


  def getIdentifiers(self, hosts, 
                     node_id=None, 
                     date_start=None, 
                     date_end=None):
    '''
    Retrieve a space delimited list of:

      PID docid rev series_id

    from the postgres database on each host of hosts. Output is written to 
    a file in dest_folder named identifier_{host}.txt. The file name is
    recorded in the results disctionary.

    The identifiers returned is limited by the combination of 
    node_id, date_start, and date_end which are ANDed together. At least one 
    of these parameters must be specified.

    :param results: Dictionary of {'request': ,'result': } to contain 
                    metadata about this operation.
                    Modified by this method.
    :param hosts: List of host names
    :param node_id: Optional node identifier to match
    :param date_start: Optional starting date for match
    :param date_end: Optional ending date for match.
    :return:
    '''
    logger = logging.getLogger('main')
    sql_template = """SELECT identifier.guid, identifier.docid, identifier.rev,
      systemmetadata.series_id, systemmetadata.origin_member_node,
      systemmetadata.object_format FROM identifier INNER JOIN systemmetadata ON
      identifier .guid = systemmetadata.guid WHERE """
    conditions = []
    params = []

    if not node_id is None:
      if node_id.find("'") >= 0:
        raise ValueError("Invalid character in node_id!")
      conditions.append("origin_member_node = '%s'" % node_id)
      params.append(node_id)

    if not date_start is None:
      conditions.append("date_modified >= '%s'::timestamp" % 
         date_start.strftime(PSQL_DATE_FORMAT))
      params.append(date_start)

    if not date_end is None:
      conditions.append("date_modified < '%s'::timestamp" %
         date_end.strftime(PSQL_DATE_FORMAT))
      params.append(date_end)
      
    if len(conditions) < 1:
      raise ValueError(
          "At least one of node_id, date_start, or date_end is required.")

    where_clause = " AND ".join(conditions)
    SQL = sql_template + where_clause;
    results['request']['SQL'] = SQL
    logger.info(SQL)
    dbc = getLocalDatabase(dest_folder)
    cur = dbc.cursor()
    setMetadata('request.SQL', SQL)
  
    for host in hosts:
      dest_file = "identifiers_{0}.txt".format(host)
      self.setMetadata("identifiers.{0}".format(host), dest_file)
      results['results']['identifier_documents'][host] = dest_file
      execute(runSQLQuery, 
              SQL, 
              os.path.join(self.dest_folder, dest_file),
              host=host)
      logger.info("Loading identifiers to local database for %s", host)
      with open( os.path.join(self.dest_folder, dest_file) ) as id_file:
        idreader = csv.reader(id_file)
        for row in idreader:
          urow =[host, ] + map( toUnicode, row)
          urow[4] = urow[2]+u'.'+unicode(urow[3])
          cur.execute(u"INSERT INTO identifier VALUES (?, ?, ?, ?, ?, ?, ?);",
              urow)    
      dbc.commit()


  def loadDataFromHosts(self, 
                        hosts,
                        node_id=None, 
                        date_start=None, 
                        date_end=None):
    self.setMetadata('hosts', hosts)
    self.getDocids(hosts)
    self.getIdentifiers(hosts, 
                        node_id=node_id, 
                        date_start=date_start, 
                        date_end=date_end)
                        
                        

  def filesHereNotThere(self, host_a, host_b):
    dbc = self.getDatabaseConnection()
    cur = dbc.cursor()
    logger = logging.getLogger('main')
    logger.info("Comparing file list for %s vs %s", host_a, host_b)
    SQL = """SELECT COUNT(*) FROM files WHERE host=? AND filename NOT IN 
    ( SELECT filename FROM files WHERE host=?);"""
    cur.execute(SQL, (host_a, host_b))
    delta = cur.fetchone()[0]
    res = {'delta': delta,
           'docids': []}
    if delta > 0:
      SQL = """SELECT filename FROM files WHERE host=? AND filename NOT IN 
      ( SELECT filename FROM files WHERE host=?);"""
      cur.execute(SQL, (host_a, host_b))
      docids = cur.fetchall()
      for entry in docids:
        res['docids'].append(entry[0])
    return res


  def docIdCounts(self):
    hosts = self.getMetadata('hosts')
    res = []
    for host_a in hosts:
      res_a = []
      for host_b in hosts:
        res_a.append(filesHereNotThere(host_a, host_b))
      res.append(res_a)
    return res
  

  def analyzeData(self):
    docid_counts = self.docIdCounts()
    self.setMetadata('docid_counts', DOCID_ANALYSIS)
    self.writeJsonFile( DOCID_ANALYSIS, docid_counts )


  def render(self, format='text'):
    if format.lower() == 'text':
      return self.renderDataText()


  def renderText(self):
    hosts = self.getMetadata('hosts')
    docid_comp = self.readJsonFile(DOCID_ANALYSIS)
    print "            " + " ".join(hosts)
    i = 0
    for host_a in hosts:
      olin = host_a.split(".")[0]
      j = 0
      for host_b in hosts:
        olin += "  {0}".format( docid_compare['comparisons'][i][j]['delta'] ) 
        j += 1
      print( olin )
      i += 1
    


##==============================================================================


def getLocalDatabase(dest_folder, db_file="cndata.sqlite"):
  dbc = sqlite3.connect( os.path.join(dest_folder, db_file ))
  return dbc


def createLocalDatabase(dest_folder, db_file="cndata.sqlite"):
  '''Initialize a local sqlite3 database to hold data for analysis
  '''
  dbc = getLocalDatabase(dest_folder, db_file)
  cur = dbc.cursor()
  cur.execute("CREATE TABLE IF NOT EXISTS metadata (k TEXT, v TEXT)")
  cur.execute("""CREATE TABLE IF NOT EXISTS identifier (host TEXT, pid TEXT,
    autogen TEXT, rev INTEGER, docid TEXT, nodeid TEXT, formatid TEXT);""")
  cur.execute("""CREATE INDEX IF NOT EXISTS idx_identifier ON identifier 
    (pid, host, docid);""")
  cur.execute("CREATE TABLE IF NOT EXISTS files (host TEXT, filename TEXT);")
  cur.execute("CREATE INDEX IF NOT EXISTS idx_files ON files (filename, host);")
  dbc.commit()
  dbc.close()


def doGetIdentifiersPSQL(results, hosts, dest_folder, node_id=None, date_start=None, date_end=None):
  '''
  Retrieve a space delimited list of:

    PID docid rev series_id

  from the postgres database on each host of hosts. Output is written to a file in dest_folder
  named identifier_{host}.txt. The file name is recorded in the results disctionary.

  The identifiers returned is limited by the combination of node_id, date_start, and date_end which
  are ANDed together. At least one of these parameters must be specified.

  :param results: Dictionary of {'request': ,'result': } to contain metadata about this operation.
                  Modified by this method.
  :param hosts: List of host names
  :param node_id: Optional node identifier to match
  :param date_start: Optional starting date for match
  :param date_end: Optional ending date for match.
  :return:
  '''
  logger = logging.getLogger('main')
  sql_template = """SELECT identifier.guid, identifier.docid, identifier.rev, systemmetadata.series_id, systemmetadata.origin_member_node, systemmetadata.object_format
FROM identifier INNER JOIN systemmetadata ON identifier .guid = systemmetadata.guid
WHERE """
  conditions = []
  params = []
  if not node_id is None:
    if node_id.find("'") >= 0:
      raise ValueError("Invalid character in node_id!")
    conditions.append("origin_member_node = '%s'" % node_id)
    params.append(node_id)
  if not date_start is None:
    conditions.append("date_modified >= '%s'::timestamp" % date_start.strftime(PSQL_DATE_FORMAT))
    params.append(date_start)
  if not date_end is None:
    conditions.append("date_modified < '%s'::timestamp" % date_end.strftime(PSQL_DATE_FORMAT))
    params.append(date_end)
  if len(conditions) < 1:
    raise ValueError("At least one of node_id, date_start, or date_end is required.")
  where_clause = " AND ".join(conditions)
  SQL = sql_template + where_clause;
  results['request']['SQL'] = SQL
  logger.info(SQL)
  results['results']['identifier_documents'] = {}
  dbc = getLocalDatabase(dest_folder)
  cur = dbc.cursor()
  
  setMetadata('request.SQL', SQL)
  
  for host in hosts:
    dest_file = os.path.join(dest_folder, "identifiers_{0}.txt".format(host))
    results['results']['identifier_documents'][host] = dest_file
    execute(runSQLQuery, SQL, dest_file, host=host)
    logger.info("Loading identifiers to local database for %s", host)
    with open(results['results']['identifier_documents'][host]) as id_file:
      idreader = csv.reader(id_file)
      for row in idreader:
        urow =[host, ] + map( toUnicode, row)
        urow[4] = urow[2]+u'.'+unicode(urow[3])
        cur.execute(u"INSERT INTO identifier VALUES (?, ?, ?, ?, ?, ?, ?);",
            urow)    
    dbc.commit()


def doGetDataDocumentList(results, hosts, dest_folder):
  '''
  Retrieve a list of files from /var/metacat/data and /var/metacat/documents from each of hosts.

  Output is written to files in dest_folder named data_files_{host}. Each output file contains one entry per line.

  :param results:
  :param hosts:
  :param dest_folder:
  :return:
  '''
  logger = logging.getLogger('main')
  results['results']['data_documents'] = {}
  dbc = getLocalDatabase(dest_folder)
  cur = dbc.cursor()
  for host in hosts:
    dest_file = os.path.join(dest_folder, "data_files_{0}.txt".format(host))
    results['results']['data_documents'][host] = dest_file
    execute(listDataDocuments, dest_file, host=host)
    logger.info("Adding document list for %s to database...", host)
    with open(dest_file, 'r') as infile:
      for row in infile.readlines(): 
        cur.execute(u"INSERT INTO files VALUES (?, ?)", 
                    [ host, toUnicode(row.strip()) ] )
    dbc.commit()


def listAnotB(A, B):
  '''
  Returns a list of elements in A that are not present in B

  :param A: set
  :param B: set
  :return: list
  '''
  return list(A.difference(B))


def toUnicode(a):
  return a.decode('utf-8')


def compareIdentifierList(results):
  '''
  Compares the identifier list for each host with that for each other host.

  :param results:
  :return:
  '''
  dbc = getLocalDatabase(results['dest_folder'])
  cur = dbc.cursor()
  logger = logging.getLogger('main')
  logger.info("Comparing identifiers...")
  pids = {}
  hosts = results['results']['identifier_documents'].keys()
  for host in hosts:
    logger.info("Loading %s ...", host)
    pids[host] = None
    tmplist = []
    with open(results['results']['identifier_documents'][host]) as id_file:
      idreader = csv.reader(id_file)
      for row in idreader:
        tmplist.append(row[0])
    pids[host] = frozenset(tmplist)
  results['results']['identifier_compare'] = []
  results['results']['identifier_compare_nodes'] = []
  for host_A in hosts:
    logger.info("Comparing %s", host_A)
    for host_B in hosts:
      logger.info("Comparing {0} - {1}".format(host_A, host_B))
      delta = {"A":host_A,
               "B":host_B,
               "delta":listAnotB(pids[host_A], pids[host_B])}
      results['results']['identifier_compare'].append(delta)
      delta_nodes = {"A": host_A,
                     "B": host_B,
                     "nodes": {}}
      for pid in delta['delta']:
        SQL = u"SELECT nodeid FROM identifier WHERE pid=? AND host=?"
        cur.execute(SQL, [toUnicode(pid), host_A])
        info = cur.fetchone()
        try:
          delta_nodes['nodes'][info[0]] += 1
        except:
          delta_nodes['nodes'][info[0]] = 1
      results['results']['identifier_compare_nodes'].append(delta_nodes)
  return pids


def compareDocIdList(results):
  '''
  Compares the docids + rev with available file names

  :param results:
  :return:
  '''
  logger = logging.getLogger('main')
  logger.info("Comparing docids...")
  docids = {}
  filenames = {}
  hosts = results['results']['identifier_documents'].keys()
  for host in hosts:
    docids[host] = None
    tmplist = []
    with open(results['results']['identifier_documents'][host], 'rb') as id_file:
      idreader = csv.reader(id_file)
      for row in idreader:
        tmplist.append("{0}.{1}".format(row[1], row[2]))
    docids[host] = frozenset(tmplist)
  for host in hosts:
    filenames[host] = None
    tmplist = []
    with open(results['results']['data_documents'][host],'rb') as id_file:
      for row in id_file.readlines():
        tmplist.append(row.strip())
    filenames[host] = frozenset(tmplist)
    logger.info("Loaded %d filenames for host %s.", len(filenames[host]), host)
  results['results']['docid_compare'] = []
  for host in hosts:
    logger.info("Comparing docids with filesystem for host {0}".format(host))
    delta = {'A': host,
             'B': 'files',
             'delta': listAnotB(docids[host], filenames[host])}
    results['results']['docid_compare'].append(delta)
  return docids


def filesHereNotThere(dbc, host_a, host_b):
  cur = dbc.cursor()
  logger = logging.getLogger('main')
  logger.info("Comparing file list for %s vs %s", host_a, host_b)
  SQL = """SELECT COUNT(*) FROM files WHERE host=? AND filename NOT IN 
  ( SELECT filename FROM files WHERE host=?);"""
  cur.execute(SQL, (host_a, host_b))
  delta = cur.fetchone()[0]
  res = {'delta': delta,
         'docids': []}
  if delta > 0:
    SQL = """SELECT filename FROM files WHERE host=? AND filename NOT IN 
    ( SELECT filename FROM files WHERE host=?);"""
    cur.execute(SQL, (host_a, host_b))
    docids = cur.fetchall()
    for entry in docids:
      res['docids'].append(entry[0])
  return res
  
  
def docIdCounts(dbc, hosts):
  res = {'hosts': hosts,
         'comparisons':[]}
  for host_a in hosts:
    res_a = []
    for host_b in hosts:
      res_a.append(filesHereNotThere(dbc, host_a, host_b))
    res['comparisons'].append(res_a)
  return res
  


def analyzeThis(results_folder, format="text", template="d1getpids.md"):
  '''
  Given the content retireved from the CNs, performs comparisons, stores and outputs the results
  :param results_folder:
  :param format:
  :return:
  '''
  #TODO: figure path dynamically
  env = Environment(
    loader=FileSystemLoader('/Users/vieglais/Projects/DataONE_PhaseII/Operations/github/DataONE_Operations/scripts/templates'), 
    autoescape=False
    )
  template = env.get_template("d1getpids.md")
  results = {}
  with codecs.open( os.path.join(results_folder, RESULTS_FILE), 
                    mode='rb', 
                    encoding='utf-8' ) as rfile:
    results = json.load(rfile, encoding='utf-8')
  results['dest_folder'] = results_folder
  hosts = results['results']['data_documents'].keys()
  if format == "text":
    print("Hosts examined:")
    for host in hosts:
      print("  {0}".format(host))
    print("")
    print("PID comparison:")
    
  
    
  identifier_check = compareIdentifierList(results)
  filename_check = compareDocIdList(results)
  
  dbc = getLocalDatabase(results['dest_folder'])
  docid_compare = docIdCounts(dbc, hosts)
  
  with codecs.open( os.path.join(results_folder, ANALYSIS_FILE), 
                    mode='wb', 
                    encoding='utf-8' ) as rfile:
    rfile.write( json.dumps(results, 
                            encoding='utf-8', 
                            sort_keys=True, indent=2 ))
  if format == "text":
    for delta in results['results']['identifier_compare']:
      print("Found {0}/{1} identifiers in {2} not in {3}".format( \
        len(delta['delta']),
        len(identifier_check[delta['A']]),
        delta['A'],
        delta['B']) 
      )
    print("")
    print("Document comparison")
    for delta in results['results']['docid_compare']:
      print("{0}/{1} filenames on {2} but files not present".format( \
        len(delta['delta']),
        len(filename_check[delta['A']]),
        delta['A'])
      )
      
    #print( template.render(hosts=hosts) )  
    
    print "            " + " ".join(hosts)
    i = 0
    for host_a in hosts:
      olin = host_a.split(".")[0]
      j = 0
      for host_b in hosts:
        olin += "  {0}".format( docid_compare['comparisons'][i][j]['delta'] ) 
        j += 1
      print( olin )
      i += 1
  elif format == "json":
    print json.dumps(results, encoding="utf-8", indent=2)


def main():
  '''
  The main operation, loads supplied arguments and does the work.

  -A --analyze_only: analyze previously retrieved results
  -d --dest_folder:  destination folder for results
  -c --config:       optional path to configuration
  -e --environment:  name of environment
  -f --format:       name of output format
  -h --help:         display some help
  -H --hosts:        comma delimited list of host names
  -l --log_level:    flag to turn on logging, more means more detailed logging.
  -n --node_id:      restrict retrieved identifiers to specified node
  -x --date_start:   earliest date of retrieved records
  -y --date_end:     latest date of retrieved records

  Dates are parsed using dateparser, so human strings like "a week ago" are valid.

  :return: int for sys.exit()
  '''
  parser = argparse.ArgumentParser(description='Retrieve identifiers from a CN direct from database.')
  parser.add_argument('-n', '--node_id',
                      default=None,
                      help='Origin Node Identifier for identifiers')
  parser.add_argument('-x', '--date_start',
                      default=None,
                      help='Starting date for records')
  parser.add_argument('-y', '--date_end',
                      default=None,
                      help='Ending date for records')
  parser.add_argument('-d', '--dest_folder',
                      default=None,
                      help="Folder for results, defaults to getpids_ENVIRONMENT_DATE.")
  parser.add_argument('-H', '--hosts',
                      default=None,
                      help='Use specified comma delimited list of nodes, overrides -e')
  parser.add_argument('-A','--analyze_only',
                      action='store_true',
                      default=False,
                      help="Analyze previously retrieved results (-d required)")
  args, config = d1_admin_tools.defaultScriptMain(parser)
  logger = logging.getLogger('main')

  dest_folder = args.dest_folder
  if dest_folder is None:
    if args.analyze_only:
      raise ValueError("dest_folder must be specified for analyze_only option.")
    tnow = datetime.datetime.utcnow()
    if args.hosts is not None:
      dest_folder = "getpids_custom_{1}".format(tnow.strftime("%Y%m%d"))
    else:
      dest_folder = "getpids_{0}_{1}".format(args.environment, tnow.strftime("%Y%m%d"))
  if args.analyze_only:
    return analyzeThis(dest_folder, format=args.format)

  if not os.path.exists(dest_folder):
    os.makedirs(dest_folder)
    createLocalDatabase(dest_folder)
  date_start = None
  date_end = None
  if args.date_start is not None:
    date_start = dateparser.parse(args.date_start)
    date_str = "None"
    if date_start is not None:
      date_str = date_start.strftime(PSQL_DATE_FORMAT)
    logger.info("date_start '%s' parsed as %s", args.date_start, date_str)
  if args.date_end is not None:
    date_end = dateparser.parse(args.date_end)
    date_str = "None"
    if date_end is not None:
      date_str = date_end.strftime(PSQL_DATE_FORMAT)
    logger.info("date_end '%s' parsed as %s", args.date_end, date_str)
  hosts = []
  if args.hosts is not None:
    hosts = args.hosts.split(',')
  else:
    hosts = config.hosts(args.environment)
  results = {"hosts": hosts,
             "request":{},
             "results":{}
             }
  #Load the identifiers for the specified constraints
  doGetIdentifiersPSQL(results,
                       hosts, 
                       dest_folder, 
                       args.node_id, 
                       date_start, 
                       date_end)
  #Load the docids from the specified hosts
  doGetDataDocumentList(results, hosts, dest_folder)
  dest_file = os.path.join(dest_folder, RESULTS_FILE)
  with codecs.open(dest_file, mode='wb', encoding='utf-8') as dfile:
    dfile.write(json.dumps(results, encoding='utf-8'))
  return analyzeThis(dest_folder, format=args.format)


if __name__ == "__main__":
  sys.exit(main())
