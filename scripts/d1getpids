#!/bin/env python

'''
Evaluate presence of content synchronized to Coordinating Nodes.

Retrieve PIDs, SIDs, docid, revisions, and file names directly from CN
databases and file system, then perform some analysis of the retrieved
content to verify consistency across CNs.

This script bypasses the DataONE APIs to retrieve identifiers directly from the
postgres database operating on Coordinating Nodes. Shell access to the CNs and
a (readonly) postgres account are required. No password is provided for here,
so access control for ssh is via a key pair, and for postgres via a .pgpass file
or similar mechanism.

By default, content is placed in a subfolder named getpids_{env}_{date} of the
current working folder, where env is the name of the environment being examined
and date is the current date (UTC).

Analyses

1. Files that are present on one node but not on another

2. PIDs that are on one node but not on another


Output

Several files are created in the output subfolder:

cndata.sqlite

An sqlite3 database that contains tables::

  CREATE TABLE metadata (
    k TEXT,
    v TEXT
  );

  CREATE TABLE identifier (
      host TEXT, 
      pid TEXT, 
      autogen TEXT,
      rev INTEGER, 
      docid TEXT, 
      nodeid TEXT, 
      formatid TEXT
  );
  CREATE TABLE files (
      host TEXT, 
      filename TEXT
  );


data_files_{HOST}.txt

Contains a list of all the autogen ids for all documents in the metacat data folder.

identifiers_{HOST}.txt

Contains a space delimited list of:

  identifier, autogen_id, revison_id, node_id

for identifiers in the metacat postgres database that match the provided constraints (a combination of start time, end time, node id).

analysis.json

results.json



Requires

* d1_admin_tools https://github.com/DataONEorg/DataONE_Operations
* dateparser https://pypi.python.org/pypi/dateparser
* fabric http://www.fabfile.org/


'''

import sys
import logging
import argparse
import d1_admin_tools
from d1_admin_tools import operations
import datetime
import dateparser
import os
import csv
import codecs
import json
import sqlite3
from fabric.api import task, execute, run, get
from jinja2 import Environment, FileSystemLoader, select_autoescape


PSQL_READONLY_USER = "dataone_readonly"
PSQL_DATE_FORMAT="%Y-%m-%dT%H:%M:%S.%f%z"
DOCID_ANALYSIS = 'docid_analysis.json'
PID_ANALYSIS = 'pid_analysis.json'

#========================
#++ markdown utilities ++

def toMdTable(header, matrix, widths=None):
  '''
  Return a markdown table from the provided header and 2d matrix
  '''
  if widths is None:
    widths = []
    for x in header:
      widths.append(u"-"*len(x))
  res = u"| " + u" | ".join(map(unicode, header)) + u" |\n"
  res += u"| " + u" | ".join(widths) + u" |\n"
  for row in matrix:
    line = u"| " + u" | ".join(map(unicode, row)) + u" |\n"
    res += line
  return res


#==================
#++ Fabric Tasks ++

def runSQLQuery(SQL, dest_file, tmp_file=None, user=PSQL_READONLY_USER, database='metacat', delimiter=' '):
  '''
  Fabric task that executes SQL on postgres and retrieves the resulting output as a file.

  :param SQL: SQL to run, expects a table output
  :param dest_file: Local file that will contain the results
  :param tmp_file: Temporary file on remote system. Created if not specified.
  :return: Nothing
  '''
  logger = logging.getLogger('main')
  if tmp_file is None:
    tmp_file = operations.tmpFileName("/tmp/", prefix="sql", ext="txt")
  SQL = "COPY (" + SQL + ") TO STDOUT WITH (FORMAT CSV, HEADER FALSE, FORCE_QUOTE *);"
  cmd = "psql -h localhost -U " + PSQL_READONLY_USER + " " + database
  cmd += " -P pager=off --single-transaction"
  cmd += " --no-align -t --field-separator '" + delimiter + "' --quiet"
  cmd += " -o " + tmp_file + " -c \"" + SQL + ";\""
  logger.info("PSQL Command = %s", cmd)
  run(cmd)
  get(tmp_file, dest_file)
  run("rm " + tmp_file)


def listDataDocuments(dest_file):
  '''
  Fabric task to retrieve a list of files managed by metacat on a CN.

  :param dest_file: File to contain results, one filename per line
  :return: Nothing
  '''
  tmp_file = operations.tmpFileName("/tmp/", prefix="metacat_files", ext="txt")
  cmd = "ls /var/metacat/documents > {0}".format(tmp_file)
  run(cmd)
  cmd = "ls /var/metacat/data >> {0}".format(tmp_file)
  run(cmd)
  get(tmp_file, dest_file)
  run("rm " + tmp_file)


#==================

class CNodeComparison(object):
  
  def __init__(self, dest_folder):
    self._L = logging.getLogger(self.__class__.__name__)
    self.dest_folder = dest_folder
    self.db_file = os.path.join(dest_folder, "cndata.sqlite")
    self.dbc = None
    self.createLocalDatabase()
    
    
  def getDatabaseConnection(self, force=False):
    if self.dbc is None or force:
      self.dbc = sqlite3.connect( self.db_file )
    return self.dbc


  def createLocalDatabase(self):
    dbc = self.getDatabaseConnection()
    cur = dbc.cursor()
    cur.execute("CREATE TABLE IF NOT EXISTS metadata (k TEXT, v TEXT)")
    cur.execute("""CREATE TABLE IF NOT EXISTS identifier (host TEXT, pid TEXT,
      autogen TEXT, rev INTEGER, docid TEXT, nodeid TEXT, formatid TEXT);""")
    cur.execute("""CREATE INDEX IF NOT EXISTS idx_identifier ON identifier 
      (pid, host, docid);""")
    cur.execute("CREATE TABLE IF NOT EXISTS files (host TEXT, filename TEXT);")
    cur.execute("""CREATE INDEX IF NOT EXISTS idx_files ON files 
      (filename, host);""")
    dbc.commit()


  def getMetadata(self, k):
    self._L.debug("getMetadata %s", k)
    dbc = self.getDatabaseConnection()
    cur = dbc.cursor()
    cur.execute("SELECT v FROM metadata WHERE k=?", [k])
    v = cur.fetchone()
    self._L.debug(unicode(v))
    try:
      return json.loads(v[0])
    except TypeError as e:
      pass
    return None


  def deleteMetadata(self, k):
    if self.getMetadata(k) is None:
      return
    SQL = "DELETE FROM metadata WHERE k=?"
    dbc = self.getDatabaseConnection()
    cur = dbc.cursor()
    cur.execute(SQL, [k] )
    dbc.commit()


  def setMetadata(self, k, v):
    self._L.debug("set metadata %s : %s", k, unicode(v))
    dbc = self.getDatabaseConnection()
    cur = dbc.cursor()
    v_encoded = json.dumps(v, encoding='utf-8')
    if self.getMetadata(k) is None:
      SQL = "INSERT INTO metadata VALUES (?, ?)"
      cur.execute(SQL, [k, v_encoded])
    else:
      SQL = "UPDATE metadata SET v=? WHERE k=?"
      cur.execute(SQL, [v_encoded, k])
    dbc.commit()


  def writeJsonFile(self, fname, data):
    with codecs.open( os.path.join(self.dest_folder, fname), 
                      mode='wb', 
                      encoding='utf-8' ) as rfile:
      rfile.write( json.dumps(data, 
                              encoding='utf-8', 
                              sort_keys=True, indent=2 ))
    
  
  def readJsonFile(self, fname):
    with codecs.open( os.path.join(self.dest_folder, fname), 
                      mode='rb', 
                      encoding='utf-8' ) as rfile:
      return json.load( rfile )


  def getDocids(self, hosts):
    '''
    Retrieve a list of files from /var/metacat/data and
      /var/metacat/documents from each of hosts.

    Output is written to files in dest_folder named data_files_{host}. 
    Each output file contains one entry per line.

    :param results:
    :param hosts:
    :param dest_folder:
    :return:
    '''
    logger = logging.getLogger('main')
    dbc = self.getDatabaseConnection()
    cur = dbc.cursor()
    for host in hosts:
      dest_file = "data_files_{0}.txt".format(host)
      self.setMetadata("docids.{0}".format(host), dest_file)
      execute(listDataDocuments, 
              os.path.join(self.dest_folder, dest_file), 
              host=host)
      logger.info("Adding document list for %s to database...", host)
      with open( os.path.join(self.dest_folder, dest_file), 'r') as infile:
        for row in infile.readlines(): 
          cur.execute(u"INSERT INTO files VALUES (?, ?)", 
                      [ host, toUnicode(row.strip()) ] )
      dbc.commit()


  def getIdentifiers(self, hosts, 
                     node_id=None, 
                     date_start=None, 
                     date_end=None):
    '''
    Retrieve a space delimited list of:

      PID docid rev series_id

    from the postgres database on each host of hosts. Output is written to 
    a file in dest_folder named identifier_{host}.txt. The file name is
    recorded in the results disctionary.

    The identifiers returned is limited by the combination of 
    node_id, date_start, and date_end which are ANDed together. At least one 
    of these parameters must be specified.

    :param results: Dictionary of {'request': ,'result': } to contain 
                    metadata about this operation.
                    Modified by this method.
    :param hosts: List of host names
    :param node_id: Optional node identifier to match
    :param date_start: Optional starting date for match
    :param date_end: Optional ending date for match.
    :return:
    '''
    sql_template = """SELECT identifier.guid, identifier.docid, identifier.rev,
      systemmetadata.series_id, systemmetadata.origin_member_node,
      systemmetadata.object_format FROM identifier INNER JOIN systemmetadata ON
      identifier .guid = systemmetadata.guid WHERE """
    conditions = []
    params = []

    if not node_id is None:
      if node_id.find("'") >= 0:
        raise ValueError("Invalid character in node_id!")
      conditions.append("origin_member_node = '%s'" % node_id)
      params.append(node_id)

    if not date_start is None:
      conditions.append("date_modified >= '%s'::timestamp" % 
         date_start.strftime(PSQL_DATE_FORMAT))
      params.append(date_start)

    if not date_end is None:
      conditions.append("date_modified < '%s'::timestamp" %
         date_end.strftime(PSQL_DATE_FORMAT))
      params.append(date_end)
      
    if len(conditions) < 1:
      raise ValueError(
          "At least one of node_id, date_start, or date_end is required.")

    where_clause = " AND ".join(conditions)
    SQL = sql_template + where_clause;
    self._L.info(SQL)
    dbc = self.getDatabaseConnection()
    cur = dbc.cursor()
    self.setMetadata('request.SQL', SQL)
  
    for host in hosts:
      dest_file = "identifiers_{0}.txt".format(host)
      self.setMetadata("identifiers.{0}".format(host), dest_file)
      execute(runSQLQuery, 
              SQL, 
              os.path.join(self.dest_folder, dest_file),
              host=host)
    for host in hosts:
      dest_file = "identifiers_{0}.txt".format(host)
      self._L.info("Loading identifiers to local database for %s", host)
      with open( os.path.join(self.dest_folder, dest_file) ) as id_file:
        idreader = csv.reader(id_file)
        for row in idreader:
          urow =[host, ] + map( toUnicode, row)
          urow[4] = urow[2]+u'.'+unicode(urow[3])
          cur.execute(u"INSERT INTO identifier VALUES (?, ?, ?, ?, ?, ?, ?);",
              urow)    
      dbc.commit()


  def loadDataFromHosts(self, 
                        hosts,
                        node_id=None, 
                        date_start=None, 
                        date_end=None):
    self.setMetadata('hosts', hosts)
    self.getDocids(hosts)
    self.getIdentifiers(hosts, 
                        node_id=node_id, 
                        date_start=date_start, 
                        date_end=date_end)
                        
                        
  def filesHereNotThere(self, host_a, host_b):
    dbc = self.getDatabaseConnection()
    cur = dbc.cursor()
    self._L.info("Comparing file list for %s vs %s", host_a, host_b)
    SQL = """SELECT COUNT(*) FROM files WHERE host=? AND filename NOT IN 
    ( SELECT filename FROM files WHERE host=?);"""
    cur.execute(SQL, (host_a, host_b))
    delta = cur.fetchone()[0]
    res = {'A': host_a,
           'B': host_b,
           'delta': delta,
           'comp': []}
    if delta > 0:
      SQL = """SELECT filename FROM files WHERE host=? AND filename NOT IN 
      ( SELECT filename FROM files WHERE host=?);"""
      cur.execute(SQL, (host_a, host_b))
      docids = cur.fetchall()
      for entry in docids:
        res['comp'].append(entry[0])
    return res


  def docIdCounts(self):
    hosts = self.getMetadata('hosts')
    res = []
    for host_a in hosts:
      res_a = []
      for host_b in hosts:
        res_a.append(self.filesHereNotThere( host_a, host_b ))
      res.append(res_a)
    return res
  
  
  def pidsHereNotThere(self, host_a, host_b ):
    dbc = self.getDatabaseConnection()
    cur = dbc.cursor()
    self._L.info("Comparing PIDs for %s vs %s", host_a, host_b)
    SQL = """SELECT COUNT(*) FROM identifier WHERE host=? AND pid NOT IN 
    ( SELECT pid FROM identifier WHERE host=?);"""
    cur.execute(SQL, (host_a, host_b))
    delta = cur.fetchone()[0]
    res = {'A': host_a,
           'B': host_b,
           'delta': delta,
           'comp': []}
    if delta > 0:
      SQL = """SELECT pid FROM identifier WHERE host=? AND pid NOT IN 
      ( SELECT pid FROM identifier WHERE host=?);"""
      cur.execute(SQL, (host_a, host_b))
      docids = cur.fetchall()
      for entry in docids:
        res['comp'].append(entry[0])
    return res
  
  
  def pidCounts(self):
    hosts = self.getMetadata('hosts')
    res = []
    for host_a in hosts:
      res_a = []
      for host_b in hosts:
        res_a.append(self.pidsHereNotThere( host_a, host_b ))
      res.append( res_a )
    return res


  def analyzeData(self):
    docid_counts = self.docIdCounts()
    self.setMetadata('docid_counts', DOCID_ANALYSIS)
    self.writeJsonFile( DOCID_ANALYSIS, docid_counts )
    docid_counts = None
    pid_counts = self.pidCounts()
    self.setMetadata('pid_counts', PID_ANALYSIS)
    self.writeJsonFile( PID_ANALYSIS, pid_counts )


  def render(self, format='text'):
    self._L.debug("rendering to format = %s", format)
    if format.lower() == 'text':
      return self.renderText()


  def renderText(self):
    
    def shrinkHostName(host):
      return host.split(".")[0]
    
    def dataToTable(hostnames, data):
      mxlen = 0
      for hn in hostnames:
        if len(hn) > mxlen:
          mxlen = len(hn)
      header = ["   "," "*mxlen] + hostnames
      matrix = []
      i = 0
      for host_a in hosts:
        olin = [" ", ]
        if i == 1:
          olin = ["A", ]
        olin.append(hostnames[i])
        j = 0
        for host_b in hosts:
          olin.append( data[i][j]['delta'] ) 
          j += 1
        matrix.append(olin)
        i += 1
      return header, matrix
      
    hosts = self.getMetadata('hosts')
    hostnames = map(shrinkHostName, hosts)
    data = self.readJsonFile(DOCID_ANALYSIS)
    header, matrix = dataToTable(hostnames, data)
    print( toMdTable(header, matrix))
    data = self.readJsonFile(PID_ANALYSIS)
    header, matrix = dataToTable(hostnames, data)
    print( toMdTable(header, matrix))


##==============================================================================


def main():
  '''
  The main operation, loads supplied arguments and does the work.

  -A --analyze_only: analyze previously retrieved results
  -d --dest_folder:  destination folder for results
  -c --config:       optional path to configuration
  -e --environment:  name of environment
  -f --format:       name of output format
  -h --help:         display some help
  -H --hosts:        comma delimited list of host names
  -l --log_level:    flag to turn on logging, more means more detailed logging.
  -n --node_id:      restrict retrieved identifiers to specified node
  -x --date_start:   earliest date of retrieved records
  -y --date_end:     latest date of retrieved records

  Dates are parsed using dateparser, so human strings like "a week ago" are valid.

  :return: int for sys.exit()
  '''
  parser = argparse.ArgumentParser(description=__doc__,
    formatter_class=argparse.RawDescriptionHelpFormatter)
  parser.add_argument('-n', '--node_id',
                      default=None,
                      help='Origin Node Identifier for identifiers')
  parser.add_argument('-x', '--date_start',
                      default=None,
                      help='Starting date for records')
  parser.add_argument('-y', '--date_end',
                      default=None,
                      help='Ending date for records')
  parser.add_argument('-d', '--dest_folder',
                      default=None,
                      help="Folder for results, defaults to getpids_ENVIRONMENT_DATE.")
  parser.add_argument('-H', '--hosts',
                      default=None,
                      help='Use specified comma delimited list of nodes, overrides -e')
  parser.add_argument('-A','--analyze_only',
                      action='store_true',
                      default=False,
                      help="Analyze previously retrieved results (-d required)")
  parser.add_argument('-R','--render_only',
                      action='store_true',
                      default=False,
                      help="Render previously analyzed results (-d required)")
  args, config = d1_admin_tools.defaultScriptMain(parser)
  logger = logging.getLogger('main')

  dest_folder = args.dest_folder
  if dest_folder is None:
    if args.analyze_only:
      logger.error("dest_folder must be specified for analyze_only option.")
      return 1
    tnow = datetime.datetime.utcnow()
    if args.hosts is not None:
      dest_folder = "getpids_custom_{1}".format(tnow.strftime("%Y%m%d"))
    else:
      dest_folder = "getpids_{0}_{1}".format( args.environment,
                                              tnow.strftime("%Y%m%d") )

  work = CNodeComparison(dest_folder)
  if args.analyze_only:
    work.analyzeData()
    return 0

  if args.render_only:
    work.render(format=args.format)
    return 0

  if not os.path.exists(dest_folder):
    os.makedirs(dest_folder)
    createLocalDatabase(dest_folder)
  date_start = None
  date_end = None
  if args.date_start is not None:
    date_start = dateparser.parse(args.date_start)
    date_str = "None"
    if date_start is not None:
      date_str = date_start.strftime(PSQL_DATE_FORMAT)
    logger.info("date_start '%s' parsed as %s", args.date_start, date_str)
  if args.date_end is not None:
    date_end = dateparser.parse(args.date_end)
    date_str = "None"
    if date_end is not None:
      date_str = date_end.strftime(PSQL_DATE_FORMAT)
    logger.info("date_end '%s' parsed as %s", args.date_end, date_str)
  hosts = []
  if args.hosts is not None:
    hosts = args.hosts.split(',')
  else:
    hosts = config.hosts(args.environment)
  results = {"hosts": hosts,
             "request":{},
             "results":{}
             }
  try:
    doGetIdentifiersPSQL(results, hosts, dest_folder, 
                         args.node_id, date_start, date_end)
  except ValueError as e:
    logger.error(e)
    return 1

  work.loadDataFromHosts(hosts, args.node_id, date_start, date_end)
  return 0
  #Load the identifiers for the specified constraints
  '''
  doGetIdentifiersPSQL(results,
                       hosts, 
                       dest_folder, 
                       args.node_id, 
                       date_start, 
                       date_end)
  #Load the docids from the specified hosts

  doGetDataDocumentList(results, hosts, dest_folder)
  dest_file = os.path.join(dest_folder, RESULTS_FILE)
  with codecs.open(dest_file, mode='wb', encoding='utf-8') as dfile:
    dfile.write(json.dumps(results, encoding='utf-8'))
  return analyzeThis(dest_folder, format=args.format)
  '''

if __name__ == "__main__":
  sys.exit(main())
